{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA20001 Deep Learning - Group Project\n",
    "## Image project\n",
    "\n",
    "**Due Thursday, May 20, before 23:59.**\n",
    "\n",
    "The task is to learn to assign the correct labels to a set of images.  The images are originally from a photo-sharing site and released under Creative Commons-licenses allowing sharing.  The training set contains 20 000 images. We have resized them and cropped them to 128x128 to make the task a bit more manageable.\n",
    "\n",
    "We're only giving you the code for downloading the data. The rest you'll have to do yourselves.\n",
    "\n",
    "Some comments and hints particular to the image project:\n",
    "\n",
    "- One image may belong to many classes in this problem, i.e., it's a multi-label classification problem. In fact there are images that don't belong to any of our classes, and you should also be able to handle these correctly. Pay careful attention to how you design the outputs of the network (e.g., what activation to use) and what loss function should be used.\n",
    "\n",
    "- As the dataset is pretty imbalanced, don't focus too strictly on the outputs being probabilistic. (Meaning that the right threshold for selecting the label might not be 0.5.)\n",
    "\n",
    "- Image files can be loaded as numpy matrices for example using `imread` from `matplotlib.pyplot`. Most images are color, but a few grayscale. You need to handle the grayscale ones somehow as they would have a different number of color channels (depth) than the color ones.\n",
    "\n",
    "- In the exercises we used e.g., `torchvision.datasets.MNIST` to handle the loading of the data in suitable batches. Here, you need to handle the dataloading yourself.  The easiest way is probably to create a custom `Dataset`. [See for example here for a tutorial](https://github.com/utkuozbulak/pytorch-custom-dataset-examples)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# automatically reload dependencies and repository content so that kernel need not be restarted\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from os.path import join\n",
    "from os.path import abspath\n",
    "from os.path import split\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision.datasets.utils import download_url\n",
    "import zipfile\n",
    "\n",
    "root_dir = os.getcwd()\n",
    "if root_dir not in sys.path:\n",
    "    sys.path.append(root_dir)\n",
    "    \n",
    "train_path = 'train'\n",
    "\n",
    "data_folder_name = 'image-training-corpus+annotations'\n",
    "DATA_FOLDER_DIR = os.path.abspath(os.path.join(root_dir, data_folder_name))\n",
    "\n",
    "data_zip_name = 'dl2018-image-proj.zip'\n",
    "DATA_ZIP_DIR = os.path.abspath(os.path.join(DATA_FOLDER_DIR, data_zip_name))\n",
    "\n",
    "with zipfile.ZipFile(DATA_ZIP_DIR) as zip_f:\n",
    "    zip_f.extractall(train_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above command downloaded and extracted the data files into the `train` subdirectory.\n",
    "\n",
    "The images can be found in `train/images`, and are named as `im1.jpg`, `im2.jpg` and so on until `im20000.jpg`.\n",
    "\n",
    "The class labels, or annotations, can be found in `train/annotations` as `CLASSNAME.txt`, where CLASSNAME is one of the fourteen classes: *baby, bird, car, clouds, dog, female, flower, male, night, people, portrait, river, sea,* and *tree*.\n",
    "\n",
    "Each annotation file is a simple text file that lists the images that depict that class, one per line. The images are listed with their number, not the full filename. For example `5969` refers to the image `im5969.jpg`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your stuff goes here ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "device = None\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Using GPU\")\n",
    "    device = torch.device(\"cuda:0\")\n",
    "else:\n",
    "    print(\"Using CPU\")\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from image_dataset import ImageDataset\n",
    "from data_augmentation import DataAugmentation\n",
    "\n",
    "dataAugmentation = DataAugmentation()\n",
    "dataset = ImageDataset(dataAugmentation = dataAugmentation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainIds = range(1,19001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_balancer import DataBalancer\n",
    "\n",
    "dataBalancer = DataBalancer()\n",
    "\n",
    "trainIds = dataBalancer.balanceData(trainIds, 20000-19000, trainIds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDataset = ImageDataset(trainIds, dataAugmentation=dataAugmentation)\n",
    "valDataset = ImageDataset(range(19001,20001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inception 1 iteration 1 layer output dimensions 60 * 112 * 112 = 752640\n",
      "inception 2 iteration 1 layer output dimensions 240 * 56 * 56 = 752640\n",
      "inception 3 iteration 1 layer output dimensions 48 * 56 * 56 = 150528\n",
      "inception 4 iteration 1 layer output dimensions 192 * 28 * 28 = 150528\n",
      "inception 5 iteration 1 layer output dimensions 39 * 28 * 28 = 30576\n",
      "inception 6 iteration 1 layer output dimensions 156 * 14 * 14 = 30576\n",
      "inception 7 iteration 1 layer output dimensions 32 * 14 * 14 = 6272\n",
      "inception 8 iteration 1 layer output dimensions 128 * 7 * 7 = 6272\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "class ReseptionNet(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(ReseptionNet, self).__init__()\n",
    "        \n",
    "        channels = config[\"inChannels\"]\n",
    "        \n",
    "        self.inceptions = nn.ModuleList([])\n",
    "        dimensions = config[\"inDimensions\"]\n",
    "        for i, inception in enumerate(config[\"inceptions\"]):\n",
    "            for j in range(inception[\"amount\"]):\n",
    "                inceptionLayer = Inception(channels, dimensions, inception[\"config\"])\n",
    "                channels = inceptionLayer.outChannels\n",
    "                dimensions = inceptionLayer.outDimensions\n",
    "                self.inceptions.append(inceptionLayer)\n",
    "                print(\"inception {} iteration {} layer output dimensions {} * {} * {} = {}\".format(i+1, j+1, channels, dimensions[0], dimensions[1], channels*dimensions[0]*dimensions[1]))\n",
    "\n",
    "                \n",
    "        self.flatten = Flatten()\n",
    "        self.linear = nn.Linear(channels*dimensions[0]*dimensions[1], config[\"outputs\"])\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = x\n",
    "        for inception in self.inceptions:\n",
    "            output = inception(output)\n",
    "            \n",
    "        output = self.flatten(output)\n",
    "        output = self.linear(output)\n",
    "        output = self.sigmoid(output)\n",
    "        \n",
    "        return output\n",
    "\n",
    "    \n",
    "class Inception(nn.Module):\n",
    "    \n",
    "    inceptionConfig = None\n",
    "    outChannels = None\n",
    "    outDimensions = None\n",
    "\n",
    "    def __init__(self, inChannels, inDimensions, inceptionConfig):\n",
    "        super(Inception, self).__init__()\n",
    "        self.inceptionConfig = inceptionConfig\n",
    "        \n",
    "        self.outChannels = 0\n",
    "        self.branches = nn.ModuleList([])\n",
    "        branchDimensions = [\n",
    "            self.updateDimensions(\n",
    "                inDimensions,\n",
    "                self.inceptionConfig[\"shortcut\"][\"padding\"],\n",
    "                self.inceptionConfig[\"shortcut\"][\"dilation\"],\n",
    "                self.inceptionConfig[\"shortcut\"][\"kernelSize\"],\n",
    "                self.inceptionConfig[\"shortcut\"][\"stride\"]\n",
    "            )\n",
    "        ]\n",
    "        for branch in self.inceptionConfig[\"branches\"]:\n",
    "            blocks = nn.ModuleList([])\n",
    "            channels = inChannels\n",
    "            dimensions = inDimensions\n",
    "            for block in branch[\"blocks\"]:\n",
    "                convolution = block[\"convolution\"]\n",
    "                blocks.append(convolution(\n",
    "                    channels,\n",
    "                    math.ceil(channels*block[\"outputChannelMultiplier\"]),\n",
    "                    kernel_size = block[\"kernelSize\"],\n",
    "                    padding=block[\"padding\"],\n",
    "                    stride=block[\"stride\"],\n",
    "                    dilation=block[\"dilation\"]\n",
    "                ))\n",
    "                channels = math.ceil(channels*block[\"outputChannelMultiplier\"])\n",
    "                dimensions = self.updateDimensions(dimensions, block[\"padding\"], block[\"dilation\"], block[\"kernelSize\"], block[\"stride\"])\n",
    "            self.outChannels += channels\n",
    "            self.branches.append(blocks)\n",
    "            branchDimensions.append(dimensions)\n",
    "            \n",
    "        for dimensions in branchDimensions:\n",
    "            if dimensions != branchDimensions[0]:\n",
    "                print(branchDimensions)\n",
    "                raise Exception(\"Dimensions must stay the same between all branches and shortcut in inceptions\")\n",
    "        \n",
    "        self.outDimensions = branchDimensions[0]\n",
    "            \n",
    "        self.shortcut = self.inceptionConfig[\"shortcut\"][\"convolution\"](\n",
    "            inChannels,\n",
    "            self.outChannels,\n",
    "            kernel_size = self.inceptionConfig[\"shortcut\"][\"kernelSize\"],\n",
    "            padding = self.inceptionConfig[\"shortcut\"][\"padding\"],\n",
    "            stride = self.inceptionConfig[\"shortcut\"][\"stride\"],\n",
    "            dilation = self.inceptionConfig[\"shortcut\"][\"dilation\"]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = []\n",
    "        for branch in self.branches:\n",
    "            output = x\n",
    "            for block in branch:\n",
    "                output = block(output)\n",
    "            outputs.append(output)\n",
    "        \n",
    "        output = torch.cat(outputs, 1)\n",
    "        shortcut = self.shortcut(x)\n",
    "        output = output + shortcut\n",
    "        output = F.relu(output)\n",
    "\n",
    "        return output\n",
    "    \n",
    "    def updateDimensions(self, dimensions, padding, dilation, kernelSize, stride):\n",
    "        def dimensionalize(value):\n",
    "            if type(value) is tuple:\n",
    "                return value\n",
    "            else:\n",
    "                return (value, value)\n",
    "        padding = dimensionalize(padding)\n",
    "        dilation = dimensionalize(dilation)\n",
    "        kernelSize = dimensionalize(kernelSize)\n",
    "        stride = dimensionalize(stride)\n",
    "        \n",
    "        newHeight = (dimensions[0] + 2*padding[0] - dilation[0]*(kernelSize[0]-1)-1)//(stride[0])+1\n",
    "        newWidth = (dimensions[1] + 2*padding[1] - dilation[1]*(kernelSize[1]-1)-1)//(stride[1])+1\n",
    "        return (newHeight, newWidth)\n",
    "\n",
    "class BasicConv2d(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, **kwargs):\n",
    "        super(BasicConv2d, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, bias=False, **kwargs)\n",
    "        self.bn = nn.BatchNorm2d(out_channels, eps=0.001)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        return F.relu(x, inplace=True)\n",
    "    \n",
    "class AveragePooling(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels, **kwarks):\n",
    "        super(AveragePooling, self).__init__()\n",
    "        del kwarks[\"dilation\"]\n",
    "        self.kwarks = kwarks\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return F.avg_pool2d(x, **self.kwarks)\n",
    "    \n",
    "class MaxPooling(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels, **kwarks):\n",
    "        super(MaxPooling, self).__init__()\n",
    "        del kwarks[\"dilation\"]\n",
    "        self.kwarks = kwarks\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return F.max_pool2d(x, **self.kwarks)\n",
    "    \n",
    "    \n",
    "class Flatten(nn.Module):\n",
    "    \n",
    "    def forward(self, x):\n",
    "        N, C, H, W = x.size() # read in N, C, H, W\n",
    "        return x.view(N, -1)\n",
    "    \n",
    "    \n",
    "class IdentityConv2d(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels, **kwargs):\n",
    "        super(IdentityConv2d, self).__init__()\n",
    "        self.kwarks = kwarks\n",
    "        def dimensionalize(value):\n",
    "            if type(value) is tuple:\n",
    "                return value\n",
    "            else:\n",
    "                return (value, value)\n",
    "        kernelSize = dimensionalize(kwarks[\"kernelSize\"])\n",
    "        self.weights = np.zeros(kernelSize)\n",
    "        self.weights[kernelSize[0]//2, kernelSize[1]//2] = 1\n",
    "        self.weights = torch.Tensor(self.weights)\n",
    "        self.weights = self.weights.view(1, 1, kernelSize[0], kernelSize[1]).repeat(1, out_channels, 1, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return F.conv2d(x, self.weights, **self.kwarks)\n",
    "    \n",
    "\n",
    "channelExploder = {\n",
    "    \"branches\": [\n",
    "        {\n",
    "            \"blocks\": [\n",
    "                {   \n",
    "                    \"convolution\": BasicConv2d,\n",
    "                    \"outputChannelMultiplier\": 5,\n",
    "                    \"kernelSize\": 1,\n",
    "                    \"padding\": 0,\n",
    "                    \"stride\": 1,\n",
    "                    \"dilation\": 1\n",
    "                },\n",
    "                {   \n",
    "                    \"convolution\": MaxPooling,\n",
    "                    \"outputChannelMultiplier\": 1,\n",
    "                    \"kernelSize\": 2,\n",
    "                    \"padding\": 0,\n",
    "                    \"stride\": 2,\n",
    "                    \"dilation\": 1\n",
    "                }\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"blocks\": [\n",
    "                {\n",
    "                    \"convolution\": BasicConv2d,\n",
    "                    \"outputChannelMultiplier\": 5,\n",
    "                    \"kernelSize\": 3,\n",
    "                    \"padding\": 1,\n",
    "                    \"stride\": 1,\n",
    "                    \"dilation\": 1\n",
    "                },\n",
    "                {   \n",
    "                    \"convolution\": MaxPooling,\n",
    "                    \"outputChannelMultiplier\": 1,\n",
    "                    \"kernelSize\": 2,\n",
    "                    \"padding\": 0,\n",
    "                    \"stride\": 2,\n",
    "                    \"dilation\": 1\n",
    "                }\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"blocks\": [\n",
    "                {\n",
    "                    \"convolution\": BasicConv2d,\n",
    "                    \"outputChannelMultiplier\": 5,\n",
    "                    \"kernelSize\": 5,\n",
    "                    \"padding\": 2,\n",
    "                    \"stride\": 1,\n",
    "                    \"dilation\": 1\n",
    "                },\n",
    "                {   \n",
    "                    \"convolution\": MaxPooling,\n",
    "                    \"outputChannelMultiplier\": 1,\n",
    "                    \"kernelSize\": 2,\n",
    "                    \"padding\": 0,\n",
    "                    \"stride\": 2,\n",
    "                    \"dilation\": 1\n",
    "                }\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"blocks\": [\n",
    "                {\n",
    "                    \"convolution\": BasicConv2d,\n",
    "                    \"outputChannelMultiplier\": 5,\n",
    "                    \"kernelSize\": 7,\n",
    "                    \"padding\": 3,\n",
    "                    \"stride\": 1,\n",
    "                    \"dilation\": 1\n",
    "                },\n",
    "                {   \n",
    "                    \"convolution\": MaxPooling,\n",
    "                    \"outputChannelMultiplier\": 1,\n",
    "                    \"kernelSize\": 2,\n",
    "                    \"padding\": 0,\n",
    "                    \"stride\": 2,\n",
    "                    \"dilation\": 1\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "    ],\n",
    "    \"shortcut\": {\n",
    "        \"convolution\": IdentityConv2d,\n",
    "        \"kernelSize\": 1,\n",
    "        \"padding\": 0,\n",
    "        \"stride\": 2,\n",
    "        \"dilation\": 1\n",
    "    }\n",
    "}\n",
    "\n",
    "basicInceptionConfig = {\n",
    "    \"branches\": [\n",
    "        {\n",
    "            \"blocks\": [\n",
    "                {   \n",
    "                    \"convolution\": BasicConv2d,\n",
    "                    \"outputChannelMultiplier\": 1,\n",
    "                    \"kernelSize\": 1,\n",
    "                    \"padding\": 0,\n",
    "                    \"stride\": 1,\n",
    "                    \"dilation\": 1\n",
    "                },\n",
    "                {   \n",
    "                    \"convolution\": MaxPooling,\n",
    "                    \"outputChannelMultiplier\": 1,\n",
    "                    \"kernelSize\": 2,\n",
    "                    \"padding\": 0,\n",
    "                    \"stride\": 2,\n",
    "                    \"dilation\": 1\n",
    "                }\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"blocks\": [\n",
    "                {\n",
    "                    \"convolution\": BasicConv2d,\n",
    "                    \"outputChannelMultiplier\": 1,\n",
    "                    \"kernelSize\": 3,\n",
    "                    \"padding\": 1,\n",
    "                    \"stride\": 1,\n",
    "                    \"dilation\": 1\n",
    "                },\n",
    "                {   \n",
    "                    \"convolution\": MaxPooling,\n",
    "                    \"outputChannelMultiplier\": 1,\n",
    "                    \"kernelSize\": 2,\n",
    "                    \"padding\": 0,\n",
    "                    \"stride\": 2,\n",
    "                    \"dilation\": 1\n",
    "                }\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"blocks\": [\n",
    "                {\n",
    "                    \"convolution\": BasicConv2d,\n",
    "                    \"outputChannelMultiplier\": 1,\n",
    "                    \"kernelSize\": 5,\n",
    "                    \"padding\": 2,\n",
    "                    \"stride\": 1,\n",
    "                    \"dilation\": 1\n",
    "                },\n",
    "                {   \n",
    "                    \"convolution\": MaxPooling,\n",
    "                    \"outputChannelMultiplier\": 1,\n",
    "                    \"kernelSize\": 2,\n",
    "                    \"padding\": 0,\n",
    "                    \"stride\": 2,\n",
    "                    \"dilation\": 1\n",
    "                }\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"blocks\": [\n",
    "                {\n",
    "                    \"convolution\": BasicConv2d,\n",
    "                    \"outputChannelMultiplier\": 1,\n",
    "                    \"kernelSize\": 7,\n",
    "                    \"padding\": 3,\n",
    "                    \"stride\": 1,\n",
    "                    \"dilation\": 1\n",
    "                },\n",
    "                {   \n",
    "                    \"convolution\": MaxPooling,\n",
    "                    \"outputChannelMultiplier\": 1,\n",
    "                    \"kernelSize\": 2,\n",
    "                    \"padding\": 0,\n",
    "                    \"stride\": 2,\n",
    "                    \"dilation\": 1\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "    ],\n",
    "    \"shortcut\": {\n",
    "        \"convolution\": IdentityConv2d,\n",
    "        \"kernelSize\": 1,\n",
    "        \"padding\": 0,\n",
    "        \"stride\": 2,\n",
    "        \"dilation\": 1\n",
    "    }\n",
    "}\n",
    "\n",
    "channelReducerConfig = {\n",
    "    \"branches\": [\n",
    "        {\n",
    "            \"blocks\": [\n",
    "                {   \n",
    "                    \"convolution\": BasicConv2d,\n",
    "                    \"outputChannelMultiplier\": 0.2,\n",
    "                    \"kernelSize\": 1,\n",
    "                    \"padding\": 0,\n",
    "                    \"stride\": 1,\n",
    "                    \"dilation\": 1\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ],\n",
    "    \"shortcut\": {\n",
    "        \"convolution\": IdentityConv2d,\n",
    "        \"kernelSize\": 1,\n",
    "        \"padding\": 0,\n",
    "        \"stride\": 1,\n",
    "        \"dilation\": 1\n",
    "    }\n",
    "}\n",
    "    \n",
    "config = {\n",
    "    \"inceptions\": [\n",
    "        {\n",
    "            \"config\": channelExploder,\n",
    "            \"amount\": 1,\n",
    "        },\n",
    "        {\n",
    "            \"config\": basicInceptionConfig,\n",
    "            \"amount\": 1,\n",
    "        },\n",
    "        {\n",
    "            \"config\": channelReducerConfig,\n",
    "            \"amount\": 1\n",
    "        },\n",
    "        {\n",
    "            \"config\": basicInceptionConfig,\n",
    "            \"amount\": 1,\n",
    "        },\n",
    "        {\n",
    "            \"config\": channelReducerConfig,\n",
    "            \"amount\": 1\n",
    "        },\n",
    "        {\n",
    "            \"config\": basicInceptionConfig,\n",
    "            \"amount\": 1,\n",
    "        },\n",
    "        {\n",
    "            \"config\": channelReducerConfig,\n",
    "            \"amount\": 1\n",
    "        },\n",
    "        {\n",
    "            \"config\": basicInceptionConfig,\n",
    "            \"amount\": 1,\n",
    "        }\n",
    "    ],\n",
    "    \"inChannels\": 3,\n",
    "    \"inDimensions\": (224, 224),\n",
    "    \"outputs\": 14,\n",
    "}\n",
    "\n",
    "model = ReseptionNet(config)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "----------\n",
      "Training...\n",
      "\n",
      "Progress: 0%\n",
      "Progress: 10%\n",
      "Progress: 20%\n",
      "Progress: 30%\n",
      "Progress: 40%\n",
      "Progress: 50%\n",
      "Progress: 60%\n",
      "Progress: 70%\n",
      "Progress: 80%\n",
      "Progress: 90%\n",
      "Progress: 100%\n",
      "\n",
      "train Loss: 0.2833 Acc: 0.9251\n",
      "Validating...\n",
      "\n",
      "Progress: 0%\n",
      "Progress: 10%\n",
      "Progress: 20%\n",
      "Progress: 30%\n",
      "Progress: 40%\n",
      "Progress: 50%\n",
      "Progress: 60%\n",
      "Progress: 70%\n",
      "Progress: 80%\n",
      "Progress: 90%\n",
      "Progress: 100%\n",
      "\n",
      "val Loss: 0.2280 Acc: 0.9287\n",
      "\n",
      "Training complete in 34m 45s\n",
      "Best val Acc: 0.928714\n"
     ]
    }
   ],
   "source": [
    "from train_model import train_model\n",
    "\n",
    "model = train_model(\n",
    "    model,\n",
    "    trainDataset,\n",
    "    valDataset,\n",
    "    device,\n",
    "    numberOfEpochs = 1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save your model\n",
    "\n",
    "It might be useful to save your model if you want to continue your work later, or use it for inference later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'deep_model.pkl')\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model file should now be visible in the \"Home\" screen of the jupyter notebooks interface.  There you should be able to select it and press \"download\".  [See more here on how to load the model back](https://github.com/pytorch/pytorch/blob/761d6799beb3afa03657a71776412a2171ee7533/docs/source/notes/serialization.rst) if you want to continue training later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(\"deep_model.pkl\"))\n",
    "valDataset = ImageDataset(range(19000,20000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-8b2da33d8f46>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0meval_model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0myHats\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0myTrues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0meval_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalDataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mmetric\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'precision'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'recall'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'f1'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'accuracy'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"{}: {}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmetric\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meval_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_metric\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0myTrues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0myHats\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\githubs\\Deep-image-recognition-university-project\\eval_model.py\u001b[0m in \u001b[0;36mtest_model\u001b[1;34m(model, dataset, numberOfLabels, decisionThreshold)\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"imageTensor\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"labelsTensor\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m         \u001b[0mpred_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mdecisionThresholds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0my_hat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpred_labels\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 550\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    551\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-18-02531d8212a7>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0minception\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minceptions\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m             \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 550\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    551\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-18-02531d8212a7>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    101\u001b[0m             \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mblock\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mbranch\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 103\u001b[1;33m                 \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mblock\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    104\u001b[0m             \u001b[0moutputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 550\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    551\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-18-02531d8212a7>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    134\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    135\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 136\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    137\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    138\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 550\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    551\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    347\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    348\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 349\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    350\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    351\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[1;34m(self, input, weight)\u001b[0m\n\u001b[0;32m    344\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m    345\u001b[0m         return F.conv2d(input, weight, self.bias, self.stride,\n\u001b[1;32m--> 346\u001b[1;33m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[0;32m    347\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    348\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same"
     ]
    }
   ],
   "source": [
    "import eval_model\n",
    "\n",
    "yHats, yTrues = eval_model.test_model(model, valDataset)\n",
    "for metric in ['precision', 'recall', 'f1', 'accuracy']:\n",
    "    print(\"{}: {}\".format(metric, eval_model.get_metric(yTrues, yHats, metric)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download test set\n",
    "\n",
    "The testset will be made available during the last week before the deadline and can be downloaded in the same way as the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict for test set\n",
    "\n",
    "You should return your predictions for the test set in a plain text file.  The text file contains one row for each test set image.  Each row contains a binary prediction for each label (separated by a single space), 1 if it's present in the image, and 0 if not. The order of the labels is as follows (alphabetic order of the label names):\n",
    "\n",
    "    baby bird car clouds dog female flower male night people portrait river sea tree\n",
    "\n",
    "An example row could like like this if your system predicts the presense of a bird and clouds:\n",
    "\n",
    "    0 1 0 1 0 0 0 0 0 0 0 0 0 0\n",
    "    \n",
    "The order of the rows should be according to the numeric order of the image numbers.  In the test set, this means that the first row refers to image `im20001.jpg`, the second to `im20002.jpg`, and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have the prediction output matrix prepared in `y` you can use the following function to save it to a text file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('results.txt', y, fmt='%d')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
